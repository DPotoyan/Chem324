{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Algebra 1: Basics "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectors, what are they?\n",
    "\n",
    "<html>\n",
    "<iframe width=\"300 height=\"300\" src=\"https://www.youtube.com/embed/fNk_zzaMoSs\" frameborder=\"0\" allowfullscreen>\n",
    "</iframe>\n",
    "</html>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# Visualizing geometric vs abstract representations of vectors\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Basis vectors and sample vector\n",
    "e1 = np.array([1, 0])\n",
    "e2 = np.array([0, 1])\n",
    "v = 2*e1 + 3*e2\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7, 5))\n",
    "ax.set_xlim(-0.5, 3.5)\n",
    "ax.set_ylim(-0.5, 3.5)\n",
    "ax.set_aspect('equal')\n",
    "\n",
    "# Draw basis vectors\n",
    "ax.arrow(0, 0, *e1, head_width=0.1, color='gray', length_includes_head=True)\n",
    "ax.arrow(0, 0, *e2, head_width=0.1, color='gray', length_includes_head=True)\n",
    "ax.text(1.1, -0.2, r'$\\mathbf{e_1}$', fontsize=14)\n",
    "ax.text(-0.2, 1.1, r'$\\mathbf{e_2}$', fontsize=14)\n",
    "\n",
    "# Draw vector v and its components\n",
    "ax.arrow(0, 0, *v, head_width=0.15, color='royalblue', length_includes_head=True)\n",
    "ax.plot([0, v[0]], [v[1], v[1]], '--', color='gray')\n",
    "ax.plot([v[0], v[0]], [0, v[1]], '--', color='gray')\n",
    "ax.text(v[0]+0.1, v[1]/2, r'$2\\mathbf{e_1}$', color='gray', fontsize=12, rotation=0)\n",
    "ax.text(v[0]/2, v[1]+0.1, r'$3\\mathbf{e_2}$', color='gray', fontsize=12, rotation=0)\n",
    "ax.text(v[0]+0.1, v[1]+0.1, r'$\\mathbf{v} = 2\\mathbf{e_1} + 3\\mathbf{e_2}$', color='royalblue', fontsize=14)\n",
    "\n",
    "# Axes and labels\n",
    "ax.axhline(0, color='black', lw=1)\n",
    "ax.axvline(0, color='black', lw=1)\n",
    "ax.set_xlabel('x-component')\n",
    "ax.set_ylabel('y-component')\n",
    "ax.set_title('Geometric vs Abstract Representations of a Vector', fontsize=13)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Display the abstract forms\n",
    "from IPython.display import Markdown\n",
    "Markdown(r\"\"\"\n",
    "**Abstract Representations:**\n",
    "\n",
    "$$\n",
    "\\mathbf{v} =\n",
    "\\begin{pmatrix} 2 \\\\ 3 \\end{pmatrix}\n",
    "\\quad \\Longleftrightarrow \\quad\n",
    "|v\\rangle = 2|e_1\\rangle + 3|e_2\\rangle\n",
    "$$\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectors as Arrays of Numbers\n",
    "\n",
    "We will define **vector**  as an ordered collection of numbers which describes states of physical systems. \n",
    "\n",
    "  - $a = (-2, 8)$: A 2D vector.\n",
    "  - $b = (1.34, 4.23, 5.98)$: A 3D vector.\n",
    "  - $c = (1, -2, 4i, 3 + 2i)$: A 4D vector with complex components.\n",
    "  - $f = (1, 2, 3, 4, 5, 6, \\ldots, \\infty)$: An infinite-dimensional vector with integer components.\n",
    "\n",
    "- **Note**: Vectors can belong to real or complex spaces, depending on their components.\n",
    "\n",
    "\n",
    "### Vectors Defined with Respect to a Basis\n",
    "\n",
    "- In classical physics, vectors are often visualized as arrows in space, with both **magnitude** and **direction**. A vector is typically defined with respect to a **basis**, where unit vectors ($\\vec{e_i}$) span the space. For instance, in 3D Euclidean space, the standard basis is:\n",
    "\n",
    "  $$e_1 = (1, 0, 0), \\,\\, e_2 = (0, 1, 0), \\,\\, e_3 = (0, 0, 1)$$\n",
    "\n",
    "- Here is an example of vector $v$ in terms of unit vectors of euclidean space.\n",
    "\n",
    "  $$v = (2,3,4) = 2e_1+3e_2+4e_3$$\n",
    "\n",
    "- In a different coordinate system or basis, the same vector will have different components, but its intrinsic properties remain unchanged. This flexibility in representation is a fundamental concept in quantum mechanics and linear algebra.\n",
    "\n",
    "### Row vectors vs column vector\n",
    "\n",
    "- We will defined row and column vectors which are related to one another by operation of transpose. They describe the same physical state or coordinate of a system.\n",
    "\n",
    "$$\n",
    "\\begin{pmatrix} a_1 \\\\ a_2 \\end{pmatrix} = (a_1, a_2)^{T}\n",
    "$$\n",
    "\n",
    "- However row vectors live with their own universe of row vectors and column vectors in their own universe of column vectors.\n",
    "\n",
    "- Later we will see that to take scalar or dot product row vector needs to be hit with column and vice versa.\n",
    "\n",
    "### Example: Vector expressed in two different basis\n",
    "\n",
    "Let’s take a 2D vector $\\mathbf{v}$ in the standard Cartesian unit basis:\n",
    "\n",
    "$$\n",
    "\\mathbf{v} = \\begin{pmatrix} 2 \\\\ 3 \\end{pmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathbf{v} = 2 \\mathbf{e_1} + 3 \\mathbf{e_2} = 2\\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}+3\\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "Now, suppose we switch to a new basis, where each new basis vector is a scalar multiple of the standard basis vectors. Let’s define the new basis vectors as:\n",
    "\n",
    "$$\n",
    "\\mathbf{b_1} = 2 \\mathbf{e_1} = \\begin{pmatrix} 2 \\\\ 0 \\end{pmatrix}, \\quad \\mathbf{b_2} = 2 \\mathbf{e_2} = \\begin{pmatrix} 0 \\\\ 2 \\end{pmatrix}\n",
    "$$\n",
    "\n",
    "in the new basis, the vector $\\mathbf{v}$ is represented as:\n",
    "\n",
    "$$\n",
    "\\mathbf{v}_{\\text{new}} = 1 \\mathbf{b_1} + 3/2 \\mathbf{b_2}\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "\\mathbf{v}_{\\text{new}} = \\begin{pmatrix} 1 \\\\ \\frac{3}{2} \\end{pmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "- This shows that when moving to a new basis the components of the vector change accordingly, but the vector itself remains the same geometrically!\n",
    "\n",
    "\n",
    "\n",
    "### Vector Notation\n",
    "\n",
    "Depending on the context, we may emphasize the basis or omit it when the basis is implied:\n",
    "\n",
    "  - In terms of a coordinate basis:  \n",
    "\n",
    "    $$\\vec{a} = 2\\vec{e_i} + 3\\vec{e_j}$$\n",
    "\n",
    "  - As an ordered tuple of components:  \n",
    "\n",
    "    $$a = (2, 3)$$\n",
    "\n",
    "  - Using Dirac notation (common in quantum mechanics):  \n",
    "  \n",
    "    $$\\mid a \\rangle = 2 \\mid e_i \\rangle + 3 \\mid e_j \\rangle$$\n",
    "\n",
    "- Vectors can represent a wide range of phenomena, such as the position of a particle in space, the population of countries, or temperature variations in different regions of a forest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector Operations\n",
    "\n",
    "What defines vectors in a mathematical framework are the operations performed on them. Let’s illustrate these operations using a simple 2D vectors $a = (3, 2)$ and $b=(1,1)$ as an example. \n",
    "\n",
    "#### 1. Addition or Subtraction\n",
    "\n",
    "- **Component-wise addition**:\n",
    "\n",
    "  $$a + b = \\begin{pmatrix}\n",
    "    2\\\\\n",
    "    3\n",
    "  \\end{pmatrix} + \\begin{pmatrix}\n",
    "    1\\\\\n",
    "    1\n",
    "  \\end{pmatrix} = \\begin{pmatrix}\n",
    "    2+1\\\\\n",
    "    3+1\n",
    "  \\end{pmatrix} = \\begin{pmatrix}\n",
    "    3\\\\\n",
    "    4\n",
    "  \\end{pmatrix}$$\n",
    "\n",
    "- **In Dirac notation**:\n",
    "\n",
    "  $$|a \\rangle + |b \\rangle = (2 + 1)\\mid e_1 \\rangle + (3 + 1)\\mid e_2 \\rangle$$\n",
    "\n",
    "#### 2. Multiplication by a Scalar\n",
    "\n",
    "Multiplying a vector by a scalar $\\beta=10$ scales the vector's magnitude without changing its direction. \n",
    "\n",
    "- **Component-wise scalar multiplication**:\n",
    "\n",
    "  $$10\\cdot a = 10 \\begin{pmatrix}\n",
    "    2\\\\\n",
    "    3\n",
    "  \\end{pmatrix} = \\begin{pmatrix}\n",
    "    20\\\\\n",
    "    30\n",
    "  \\end{pmatrix}$$\n",
    "\n",
    "- **In Dirac notation**:\n",
    "\n",
    "  $$10\\mid a \\rangle = 20 \\mid e_1 \\rangle + 30 \\mid e_2 \\rangle$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### System of Linear Equations as Matrix Operations\n",
    "\n",
    "A system of linear equations can be interpreted as a matrix operating on a vector to produce another vector. For example, consider the system:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "a_{11}x_1 + a_{12}x_2 &= b_1 \\\\\n",
    "a_{21}x_1 + a_{22}x_2 &= b_2\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "This system can be written compactly in matrix form as:\n",
    "\n",
    ":::{admonition} **A system of linear equation as matrix vector product**\n",
    ":class: important \n",
    "\n",
    "$$\n",
    "A \\mathbf{x} = \\mathbf{b}\n",
    "$$\n",
    "\n",
    "- $A$ is a matrix containing the coefficients of the system,\n",
    "- $\\mathbf{x}$ is the vector of unknowns, and\n",
    "- $\\mathbf{b}$ is the vector of constants (right-hand side).\n",
    "\n",
    ":::\n",
    "\n",
    "For a 2D system, this becomes:\n",
    "\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "a_{11} & a_{12} \\\\\n",
    "a_{21} & a_{22}\n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix}\n",
    "x_1 \\\\\n",
    "x_2\n",
    "\\end{pmatrix}\n",
    "=\n",
    "\\begin{pmatrix}\n",
    "b_1 \\\\\n",
    "b_2\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "### Matrix vector Multiplication\n",
    "\n",
    "**Row by column product rule**\n",
    "\n",
    "Matrix multiplication operates by taking the dot product of the rows of the matrix $A$ with the vector $\\mathbf{x}$. For the above example:\n",
    "\n",
    "$$\n",
    "\n",
    "A \\mathbf{x} = \\begin{pmatrix}\n",
    "a_{11}x_1 + a_{12}x_2 \\\\\n",
    "a_{21}x_1 + a_{22}x_2\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "This shows how the system of equations is equivalent to multiplying the matrix by the vector.\n",
    "\n",
    "\n",
    "**Lower level definition of Matrix Multiplication**\n",
    "\n",
    "In general, if $A$ is an $m \\times n$ matrix and $\\mathbf{x}$ is a vector of dimension $n$, the matrix-vector multiplication $A \\mathbf{x}$ is defined as:\n",
    "\n",
    "$$\n",
    "b_i = (A \\mathbf{x})_i = \\sum_{j=1}^{n} a_{ij} x_j\n",
    "$$\n",
    "\n",
    "Where $a_{ij}$ are the elements of matrix $A$, and $x_j$ are the components of vector $\\mathbf{x}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{admonition} **Solving linear equation via matrix Inversion**\n",
    ":class: important\n",
    "\n",
    "$${\\bf x} = A^{-1}{\\bf b}$$\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Define the coefficients matrix A and the right-hand side vector b\n",
    "print('Solving Ax=b via matrix inversion')\n",
    "\n",
    "A = np.array([[2, 3],\n",
    "              [1, -2]])\n",
    "\n",
    "b = np.array([7, 1])\n",
    "\n",
    "print('A', A)\n",
    "print('b', b)\n",
    "\n",
    "# Calculate the inverse of matrix A\n",
    "A_inv = np.linalg.inv(A)\n",
    "\n",
    "# Solve for the unknown vector x using matrix inversion: x = A_inv * b\n",
    "x = np.dot(A_inv, b)\n",
    "\n",
    "print(\"Solution using matrix inversion:\")\n",
    "print(\"x =\", x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eigenvalues and Eigenvectors\n",
    "\n",
    "- When a matrix acts on a vector, it generally **changes both its direction and length**. However, there are special vectors that **preserve their direction**—they are only **scaled** by the transformation. These are called **eigenvectors**, and the corresponding scaling factors are **eigenvalues**.\n",
    "\n",
    ":::{admonition} **Definition of Eigenvalue Problem**\n",
    ":class: important\n",
    "\n",
    "A vector $\\mathbf{v}$ is an **eigenvector** of a matrix $A$ if\n",
    "\n",
    "$$\n",
    "A \\mathbf{v} = \\lambda \\mathbf{v},\n",
    "$$\n",
    "\n",
    "where $\\lambda$ is a **scalar** called the **eigenvalue**.\n",
    ":::\n",
    "\n",
    "In this equation:\n",
    "\n",
    "* $A$ is the linear transformation (matrix or operator),\n",
    "* $\\mathbf{v}$ is a non-zero vector whose direction remains unchanged,\n",
    "* $\\lambda$ tells how much $\\mathbf{v}$ is stretched, compressed, or flipped.\n",
    "\n",
    "\n",
    "### Finding Eigenvalues\n",
    "\n",
    "Rearranging the equation gives:\n",
    "\n",
    "$$\n",
    "(A - \\lambda I)\\mathbf{v} = 0.\n",
    "$$\n",
    "\n",
    "For a nontrivial solution ($\\mathbf{v} \\neq 0$), the determinant must vanish:\n",
    "\n",
    "$$\n",
    "\\det(A - \\lambda I) = 0.\n",
    "$$\n",
    "\n",
    "This condition is the **characteristic equation** of the matrix and yields the possible eigenvalues $\\lambda$.\n",
    "\n",
    "\n",
    "### Geometric Meaning\n",
    "\n",
    "* If $\\lambda > 1$: the vector is **stretched**.\n",
    "* If $0 < \\lambda < 1$: the vector is **compressed**.\n",
    "* If $\\lambda < 0$: the vector is **flipped** in direction.\n",
    "* If $\\lambda = 0$: the vector is **collapsed** to the origin.\n",
    "\n",
    "In quantum mechanics, this idea generalizes to **operators acting on wavefunctions**—where eigenvalues correspond to **measurable quantities** (observables) and eigenvectors to the **states** with definite outcomes.\n",
    "\n",
    "\n",
    "| Property                | Expression                                       | Meaning                                            |\n",
    "| ----------------------- | ------------------------------------------------ | -------------------------------------------------- |\n",
    "| Eigenvalue equation     | $A\\mathbf{v} = \\lambda\\mathbf{v}$                | Defines scaling along eigenvector                  |\n",
    "| Inverse action          | $A^{-1}\\mathbf{v} = \\frac{1}{\\lambda}\\mathbf{v}$ | Inverse rescales in opposite way                   |\n",
    "| Invertibility condition | $\\det(A) \\neq 0$ or all $\\lambda \\neq 0$         | Matrix is invertible only if no eigenvalue is zero |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Visualizing Action of a Matrix on a Vector\n",
    "\n",
    "- These transformations illustrate different ways of modifying vectors and geometric shapes through scaling, shearing, and identity operations. We use a 2D vector and matrices as an example.\n",
    "\n",
    "| Matrix Type                  | Example                         | Effect                                        |\n",
    "| ---------------------------- | ------------------------------- | --------------------------------------------- |\n",
    "| Diagonal, unequal entries    | `[[2, 0], [0, 0.5]]`            | **Stretch** — different scaling per axis      |\n",
    "| Diagonal, equal entries      | `[[0.7, 0], [0, 0.7]]`          | **Uniform scaling** — isotropic compression   |\n",
    "| Orthogonal (rotation matrix) | `[[cosθ, -sinθ], [sinθ, cosθ]]` | **Rotation** — preserves lengths and angles   |\n",
    "| Off-diagonal elements        | `[[1, k], [0, 1]]`              | **Shear** — slants axes without changing area |\n",
    "\n",
    "- We pick the folloing four dummy vectors and act our matrix on all of them to see how the relationship between their distances and angles changes\n",
    "\n",
    "$$(0,0), (0.5,0.5), (0.5, 1.5), (0,1), (0,0)$$\n",
    "\n",
    "- We connect the points defined by this vectors to aid our visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Define the original shape (unit square)\n",
    "coords = np.array([\n",
    "    [0, 0],\n",
    "    [0.5, 0.5],\n",
    "    [0.5, 1.5],\n",
    "    [0, 1],\n",
    "    [0, 0]\n",
    "]).T\n",
    "\n",
    "# Define transformations\n",
    "stretch = np.array([[1.8, 0],\n",
    "                    [0, 0.8]])   # non-uniform diagonal → stretch\n",
    "rotation = np.array([[np.cos(np.pi/6), -np.sin(np.pi/6)],\n",
    "                     [np.sin(np.pi/6),  np.cos(np.pi/6)]])  # 30° rotation\n",
    "shear = np.array([[1, 0.8],\n",
    "                  [0, 1]])       # off-diagonal term → shear\n",
    "scaling = np.array([[0.7, 0],\n",
    "                    [0, 0.7]])   # uniform diagonal → scaling\n",
    "\n",
    "# Collect transformations\n",
    "transforms = {\n",
    "    \"Stretch (non-uniform scaling)\": stretch,\n",
    "    \"Rotation\": rotation,\n",
    "    \"Shear\": shear,\n",
    "    \"Scaling (uniform)\": scaling\n",
    "}\n",
    "\n",
    "# Set up 2x2 subplots\n",
    "fig, axes = plt.subplots(2, 2, figsize=(8, 8))\n",
    "\n",
    "for ax, (name, M) in zip(axes.flat, transforms.items()):\n",
    "    # Original shape\n",
    "    ax.plot(coords[0], coords[1], 'r--', label='original')\n",
    "    \n",
    "    # Transformed shape\n",
    "    new_coords = M @ coords\n",
    "    ax.plot(new_coords[0], new_coords[1], 'b-', label='transformed')\n",
    "    \n",
    "    # Style\n",
    "    ax.set_title(name, fontsize=11)\n",
    "    ax.set_aspect('equal', 'box')\n",
    "    ax.set_xlim(-2, 2)\n",
    "    ax.set_ylim(-1, 2)\n",
    "    ax.grid(True, ls=':')\n",
    "    ax.legend(fontsize=8, loc='upper left')\n",
    "\n",
    "plt.suptitle(\"Linear Transformations of a Vector Set\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Inner products and orthogonality\n",
    "\n",
    "- **Dot product  $\\langle a\\mid b \\rangle$** quantifies the projection of vector $a$ on $b$ and vice-versa. That is, how much $a$ and $b$ have in common with each other in terms of direction in space.  \n",
    "\n",
    "$$\n",
    "\\langle e_i \\mid e_j \\rangle =\\delta_{ij}\n",
    "$$  \n",
    "\n",
    "- **Norm of a vector $\\mid a\\mid$** Is project of the vector onto itself and quantifies the length of the vector. When the norm is $\\mid a \\mid=1$, we say that the vector is normalized.\n",
    "\t\n",
    "$$\n",
    "\\langle a \\mid a\\rangle= a_1^2+a_2^2\n",
    "$$\n",
    "\t\n",
    "$$\n",
    "\\mid a \\mid =\\sqrt{a_1^2+a_2^2}\n",
    "$$\n",
    "\n",
    "- **Orthogonality** If the projection of vector $a$ on $b$ is zero we say that the vectors are orthogonal.  Example of the orthogonal vectors are unit vectors of cartesian coordinate system. \n",
    "\n",
    "$$\n",
    "  (1,0)\\begin{pmatrix}\n",
    "    0\\\\\n",
    "    1\\\\\n",
    "    \\end{pmatrix}=1\\cdot 0+0\\cdot 1=0\n",
    "$$\n",
    "\n",
    "- **Orthonormal vectors** are both normalized and orthogonal.  We denote orthornamilty condition with the convenient Kornecker symbol: $\\delta_{ij}=0$  when $i\\neq j$ and $1$ when $i=j$. \t\n",
    "\n",
    " \n",
    "> To normalize a vector is to divide the vector by its norm. $\\mid E_1\\rangle = (4,0,0,0)$ is not normalized since $\\langle E_1\\mid E_1\\rangle = 4$ hence we divide by norm and obtain a normalized vector $\\mid e_1\\rangle=\\frac{1}{4}\\mid E_1\\rangle=(1,0,0,0)$. And now $\\langle E_1 \\mid  E_1\\rangle=1$.\n",
    "\n",
    "\n",
    "### Basis set and linear independence. \n",
    "\n",
    "**1. Every $N$-dimensional vector can be uniquely represented as a linear combination of $N$ orthogonal vectors.** And vice-versa: if a vector can be represented by $N$ orthogonal vectors, it means that the vector is $N$-dimensional. A set of vectors in terms of which an arbitrary $N$-dimensional vector is expressed is called a **basis set.**\n",
    "\n",
    "\n",
    "$$\n",
    "\\mid v\\rangle = \\sum^{i=N}_{i=1} \\mid e_i\\rangle\n",
    "$$\n",
    "\n",
    "$$a= \\begin{pmatrix}\n",
    "    2\\\\\n",
    "    3\\\\\n",
    "    \\end{pmatrix} = 2\\begin{pmatrix}\n",
    "    1\\\\\n",
    "    0\\\\\n",
    "    \\end{pmatrix}+3 \\begin{pmatrix}\n",
    "    0\\\\\n",
    "    1\\\\\n",
    "    \\end{pmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "$$a= \\begin{pmatrix}\n",
    "    -1\\\\\n",
    "    5\\\\ 8\\\\ \\end{pmatrix} = -1\\begin{pmatrix}\n",
    "    1\\\\\n",
    "    0\\\\ 0\\\\\n",
    "    \\end{pmatrix}+5 \\begin{pmatrix}\n",
    "    0\\\\\n",
    "    1\\\\ 0\\\\\n",
    "    \\end{pmatrix}+8 \\begin{pmatrix}\n",
    "    0\\\\\n",
    "    0\\\\ 1\\\\\n",
    "    \\end{pmatrix}\n",
    "$$\n",
    "    \n",
    "**2. Orthogonal vectors are linearly independent.** This means that no member of a set of vectors can be expressed in terms of the others.  Linear independence is exprsessed mathematically by having coefficients of the linear combination of 3D (4D, ND, etc) vectors to zero $\\alpha_1=\\alpha_2=\\alpha_3=0$ as the only way to satify zero vector equality: \n",
    "\n",
    "  $$\n",
    "  \\alpha_1 \\mid e_1\\rangle +\\alpha_1 \\mid e_2\\rangle+\\alpha_3 \\mid e_3\\rangle=0\n",
    "  $$   \n",
    "\n",
    "The converse, when one of the coefificent $\\alpha_i$can be non-zero immeaditely implies linear depenence,  because one can divide by that coeficient $\\alpha_i$ and express the unit vector $\\mid e_i\\rangle$ in terms of the others.\n",
    "\n",
    "### Decomposition of functions into orthogonal components\n",
    "\n",
    "- Writing a vector in terms of its orthogonal unit vectors is a powerful mathematical technique which permeates much of quantum mechanics. The role of finite dimensional vectors in QM play the  infinite dimensional functions. In analogy with sequence vectors which can live in 2D, 3D or ND spaces, the inifinite dimensional space of functions in quantum mathematics is known as a **Hilbert space**, named after famous mathematician David Hilbert. We will not go too much in depth about functional spaces other than listing some powerful analogies with simple sequence vectors.   \n",
    "\n",
    "|    Vectors                 |    Functions                  |                         \n",
    "| :----------------------------------------------------------: | :----------------------------------------------------------: |\n",
    "| **Orthonormality** $\\\\ \\langle x\\mid y \\rangle = \\sum^{i=N}_{i=1} x_i y_i=\\delta_{xy}$ | **Orthonormality** $\\\\ \\langle \\phi_i \\mid \\phi_j \\rangle = \\int^{+\\infty}_{-\\infty} \\phi_i(x) \\phi_j(x)dx=\\delta_{ij}$ |\n",
    "| **Linear superposition**  $\\\\ \\mid A \\rangle = A_x \\mid x\\rangle+A_y\\mid y\\rangle$ | **Linear superposition** $\\\\ \\mid f\\rangle = c_1 \\mid\\phi_1\\rangle+c_2\\mid\\phi_2\\rangle$ |\n",
    "| **Projections**  $\\\\ \\langle e_x\\mid A\\rangle=A_x \\langle x\\mid x \\rangle +A_y \\langle x\\mid y \\rangle=A_x  $ | **Projections** $\\\\ \\langle \\phi_1\\mid \\Psi\\rangle=c_1 \\langle \\Psi \\mid\\phi_1 \\rangle +c_2 \\langle \\Psi \\mid\\phi_2 \\rangle=c_1$ |\n",
    "\n",
    "- In the first column we decompose a vectors in terms of two orthogonal components $A_i$ or projections of vector $A$ along the orthonormal vectors $x$ and $y$.  In the second column similiar decomposition where the dot product, due to infinite dimension, is given by an integral!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
