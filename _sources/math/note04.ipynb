{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Algebra 1: Basics "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectors, what are they?\n",
    "\n",
    "<html>\n",
    "<iframe width=\"300 height=\"300\" src=\"https://www.youtube.com/embed/fNk_zzaMoSs\" frameborder=\"0\" allowfullscreen>\n",
    "</iframe>\n",
    "</html>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# Visualizing geometric vs abstract representations of vectors\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Basis vectors and sample vector\n",
    "e1 = np.array([1, 0])\n",
    "e2 = np.array([0, 1])\n",
    "v = 2*e1 + 3*e2\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7, 5))\n",
    "ax.set_xlim(-0.5, 3.5)\n",
    "ax.set_ylim(-0.5, 3.5)\n",
    "ax.set_aspect('equal')\n",
    "\n",
    "# Draw basis vectors\n",
    "ax.arrow(0, 0, *e1, head_width=0.1, color='gray', length_includes_head=True)\n",
    "ax.arrow(0, 0, *e2, head_width=0.1, color='gray', length_includes_head=True)\n",
    "ax.text(1.1, -0.2, r'$\\mathbf{e_1}$', fontsize=14)\n",
    "ax.text(-0.2, 1.1, r'$\\mathbf{e_2}$', fontsize=14)\n",
    "\n",
    "# Draw vector v and its components\n",
    "ax.arrow(0, 0, *v, head_width=0.15, color='royalblue', length_includes_head=True)\n",
    "ax.plot([0, v[0]], [v[1], v[1]], '--', color='gray')\n",
    "ax.plot([v[0], v[0]], [0, v[1]], '--', color='gray')\n",
    "ax.text(v[0]+0.1, v[1]/2, r'$2\\mathbf{e_1}$', color='gray', fontsize=12, rotation=0)\n",
    "ax.text(v[0]/2, v[1]+0.1, r'$3\\mathbf{e_2}$', color='gray', fontsize=12, rotation=0)\n",
    "ax.text(v[0]+0.1, v[1]+0.1, r'$\\mathbf{v} = 2\\mathbf{e_1} + 3\\mathbf{e_2}$', color='royalblue', fontsize=14)\n",
    "\n",
    "# Axes and labels\n",
    "ax.axhline(0, color='black', lw=1)\n",
    "ax.axvline(0, color='black', lw=1)\n",
    "ax.set_xlabel('x-component')\n",
    "ax.set_ylabel('y-component')\n",
    "ax.set_title('Geometric vs Abstract Representations of a Vector', fontsize=13)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Display the abstract forms\n",
    "from IPython.display import Markdown\n",
    "Markdown(r\"\"\"\n",
    "**Abstract Representations:**\n",
    "\n",
    "$$\n",
    "\\mathbf{v} =\n",
    "\\begin{pmatrix} 2 \\\\ 3 \\end{pmatrix}\n",
    "\\quad \\Longleftrightarrow \\quad\n",
    "|v\\rangle = 2|e_1\\rangle + 3|e_2\\rangle\n",
    "$$\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectors as Arrays of Numbers\n",
    "\n",
    "A **vector** is an ordered collection of numbers that represents the state or configuration of a physical system.\n",
    "\n",
    "* $a = (-2, 8)$ — a 2D vector\n",
    "* $e_1$ = (1,0) - a unit vector along x axis and $e_2 = (0,1)$ a unit vector along y axis\n",
    "* $b = (1.34, 4.23, 5.98)$ — a 3D vector\n",
    "* $c = (1, -2, 4i, 3 + 2i)$ — a 4D vector with complex components\n",
    "* $f = (1, 2, 3, 4, 5, 6, \\ldots)$ — an infinite-dimensional vector (e.g., a sequence)\n",
    "\n",
    "**Note:** Vectors may belong to *real* or *complex* vector spaces depending on the nature of their components.\n",
    "\n",
    "\n",
    "\n",
    ":::{admonition} **Vector Space**\n",
    ":class: tip\n",
    "\n",
    "* Any vector in an $N$-dimensional space can be written as a **linear combination** of $N$ linearly independent basis vectors.\n",
    "* For instance, in 2D space, we can choose two perpendicular unit vectors $e_1$ and $e_2$ to form a convenient basis:\n",
    "\n",
    "$$\n",
    "\\vec{a} = a_1 e_1 + a_2 e_2 = \\begin{pmatrix}\n",
    "    a_1\\\\\n",
    "    a_2\\\\\n",
    "    \\end{pmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "$$a= \\begin{pmatrix}\n",
    "    2\\\\\n",
    "    3\\\\\n",
    "    \\end{pmatrix} = 2\\begin{pmatrix}\n",
    "    1\\\\\n",
    "    0\\\\\n",
    "    \\end{pmatrix}+3 \\begin{pmatrix}\n",
    "    0\\\\\n",
    "    1\\\\\n",
    "    \\end{pmatrix}\n",
    "$$\n",
    "\n",
    "* If we choose a rotated basis $e'_1$, $e'_2$ (e.g choose basis vectors with length 2), the components change, but the **vector itself remains the same**:\n",
    "\n",
    "$$\n",
    "\\vec{a} = a'_1 e'_1 + a'_2 e'_2 = (a'_1, a'_2)\n",
    "$$\n",
    "\n",
    "\n",
    "$$a= \\begin{pmatrix}\n",
    "    2\\\\\n",
    "    3\\\\\n",
    "    \\end{pmatrix} = 1\\begin{pmatrix}\n",
    "    2\\\\\n",
    "    0\\\\\n",
    "    \\end{pmatrix}+\\frac{3}{2} \\begin{pmatrix}\n",
    "    0\\\\\n",
    "    2\\\\\n",
    "    \\end{pmatrix}\n",
    "$$\n",
    "\n",
    ":::\n",
    "\n",
    "\n",
    "\n",
    "### Row and Column Vectors\n",
    "\n",
    "- In linear algebra and quantum mechanics, we distinguish between **column vectors** (kets) and **row vectors** (bras), which are related by the **transpose** (and, in complex spaces, by the **Hermitian conjugate**):\n",
    "\n",
    "$$\n",
    "\\langle a | =\n",
    "\\begin{pmatrix}\n",
    "a_1 \\\n",
    "a_2\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "|b\\rangle =  \\begin{pmatrix}\n",
    "    b_1\\\\\n",
    "    b_2\\\\\n",
    "    \\end{pmatrix}\n",
    "$$\n",
    "\n",
    "- Although they describe the same state or object, bras and kets “live” in dual spaces — column vectors form one space, while row vectors belong to its dual space. Their interaction through the inner product $\\langle b | a \\rangle$ gives a scalar quantity.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Inner products and orthogonality\n",
    "\n",
    "- **Dot product  $\\langle a\\mid b \\rangle$** quantifies the projection of vector $a$ on $b$ and vice-versa. That is, how much $a$ and $b$ have in common in terms of direction in space!  \n",
    "\n",
    "\n",
    ":::{admonition} **Dot product: geometric definition**\n",
    ":class: important\n",
    "\n",
    "$$\n",
    "\\langle a\\mid b \\rangle = |a| |b| cos\\theta\n",
    "$$\n",
    ":::\n",
    "\n",
    "\n",
    "- The $\\theta$ measures angle between a and b. If vectors are perpendicular like unit vectors in cartesian system we call them **orthogonal** \n",
    "\n",
    "- **Orthonormal vectors** are both normalized and orthogonal.  We denote orthornamilty condition with the convenient Kornecker symbol: $\\delta_{ij}=0$  when $i\\neq j$ and $1$ when $i=j$. \t\n",
    "\n",
    "\n",
    "$$\n",
    "\\langle e_i \\mid e_j \\rangle =\\delta_{ij}\n",
    "$$  \n",
    "\n",
    "$$\n",
    "  (1,0)\\begin{pmatrix}\n",
    "    0\\\\\n",
    "    1\\\\\n",
    "    \\end{pmatrix}=1\\cdot 0+0\\cdot 1=0\n",
    "$$\n",
    "\n",
    "- **Norm of a vector $\\mid a\\mid$** Is project of the vector onto itself and quantifies the length of the vector. When the norm is $\\mid a \\mid=1$, we say that the vector is normalized.\n",
    "\t\n",
    "$$\n",
    "\\langle a \\mid a\\rangle= \\big(a_1\\langle e_1| + a_2 \\langle e_2|\\big) \\cdot \\big (a_1 |e_1\\rangle + a_2 |e_2 \\rangle \\big ) = a_1^2+a_2^2\n",
    "$$\n",
    "\t\n",
    "$$\n",
    "\\mid a \\mid =\\sqrt{a_1^2+a_2^2}\n",
    "$$\n",
    "\n",
    "\n",
    ":::{admonition} **Inner product between two vectors**\n",
    ":class: important\n",
    "\n",
    "$$\\vec{a} \\cdot \\vec{b} = \\langle a| b \\rangle = a_1 b_1+ a_2b_2+ ... $$\n",
    "\n",
    ":::\n",
    "\n",
    "\n",
    ":::{admonition} **Use inner product to defiine components**\n",
    ":class: important\n",
    "\n",
    "$$\\vec{a_1} = a_1 e_1 + a_2 \\vec{e}_2+ ...$$\n",
    "\n",
    "\n",
    "$$\\vec{e_1} \\cdot \\vec{a} = a_1$$\n",
    "\n",
    "$$\\langle e_1 | a_1\\rangle  = a_1$$\n",
    "\n",
    ":::\n",
    "\n",
    "### Decomposition of functions into orthogonal components\n",
    "\n",
    "- Writing a vector in terms of its orthogonal unit vectors is a powerful mathematical technique which permeates much of quantum mechanics. The role of finite dimensional vectors in QM play the  infinite dimensional functions. In analogy with sequence vectors which can live in 2D, 3D or ND spaces, the inifinite dimensional space of functions in quantum mathematics is known as a **Hilbert space**, named after famous mathematician David Hilbert. We will not go too much in depth about functional spaces other than listing some powerful analogies with simple sequence vectors.   \n",
    "\n",
    "|    Vectors                 |    Functions                  |                         \n",
    "| :----------------------------------------------------------: | :----------------------------------------------------------: |\n",
    "| **Orthonormality** $\\\\ \\langle x\\mid y \\rangle = \\sum^{i=N}_{i=1} x_i y_i=\\delta_{xy}$ | **Orthonormality** $\\\\ \\langle \\phi_i \\mid \\phi_j \\rangle = \\int^{+\\infty}_{-\\infty} \\phi_i(x) \\phi_j(x)dx=\\delta_{ij}$ |\n",
    "| **Linear superposition**  $\\\\ \\mid A \\rangle = A_x \\mid x\\rangle+A_y\\mid y\\rangle$ | **Linear superposition** $\\\\ \\mid f\\rangle = c_1 \\mid\\phi_1\\rangle+c_2\\mid\\phi_2\\rangle$ |\n",
    "| **Projections**  $\\\\ \\langle e_x\\mid A\\rangle=A_x \\langle x\\mid x \\rangle +A_y \\langle x\\mid y \\rangle=A_x  $ | **Projections** $\\\\ \\langle \\phi_1\\mid \\Psi\\rangle=c_1 \\langle \\Psi \\mid\\phi_1 \\rangle +c_2 \\langle \\Psi \\mid\\phi_2 \\rangle=c_1$ |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### System of Linear Equations as Matrix Operations\n",
    "\n",
    "A system of linear equations can be interpreted as a matrix operating on a vector to produce another vector. For example, consider the system:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "a_{11}x_1 + a_{12}x_2 &= b_1 \\\\\n",
    "a_{21}x_1 + a_{22}x_2 &= b_2\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "This system can be written compactly in matrix form as:\n",
    "\n",
    ":::{admonition} **A system of linear equation as matrix vector product**\n",
    ":class: important \n",
    "\n",
    "$$\n",
    "A \\mathbf{x} = \\mathbf{b}\n",
    "$$\n",
    "\n",
    "- $A$ is a matrix containing the coefficients of the system,\n",
    "- $\\mathbf{x}$ is the vector of unknowns, and\n",
    "- $\\mathbf{b}$ is the vector of constants (right-hand side).\n",
    "\n",
    ":::\n",
    "\n",
    "For a 2D system, this becomes:\n",
    "\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "a_{11} & a_{12} \\\\\n",
    "a_{21} & a_{22}\n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix}\n",
    "x_1 \\\\\n",
    "x_2\n",
    "\\end{pmatrix}\n",
    "=\n",
    "\\begin{pmatrix}\n",
    "b_1 \\\\\n",
    "b_2\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "### Matrix vector Multiplication\n",
    "\n",
    "**Row by column product rule**\n",
    "\n",
    "Matrix multiplication operates by taking the dot product of the rows of the matrix $A$ with the vector $\\mathbf{x}$. For the above example:\n",
    "\n",
    "$$\n",
    "\n",
    "A \\mathbf{x} = \\begin{pmatrix}\n",
    "a_{11}x_1 + a_{12}x_2 \\\\\n",
    "a_{21}x_1 + a_{22}x_2\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "This shows how the system of equations is equivalent to multiplying the matrix by the vector.\n",
    "\n",
    "\n",
    "**Lower level definition of Matrix Multiplication**\n",
    "\n",
    "In general, if $A$ is an $m \\times n$ matrix and $\\mathbf{x}$ is a vector of dimension $n$, the matrix-vector multiplication $A \\mathbf{x}$ is defined as:\n",
    "\n",
    "$$\n",
    "b_i = (A \\mathbf{x})_i = \\sum_{j=1}^{n} a_{ij} x_j\n",
    "$$\n",
    "\n",
    "Where $a_{ij}$ are the elements of matrix $A$, and $x_j$ are the components of vector $\\mathbf{x}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{admonition} **Solving linear equation via matrix Inversion**\n",
    ":class: important\n",
    "\n",
    "$${\\bf x} = A^{-1}{\\bf b}$$\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Define the coefficients matrix A and the right-hand side vector b\n",
    "print('Solving Ax=b via matrix inversion')\n",
    "\n",
    "A = np.array([[2, 3],\n",
    "              [1, -2]])\n",
    "\n",
    "b = np.array([7, 1])\n",
    "\n",
    "print('A', A)\n",
    "print('b', b)\n",
    "\n",
    "# Calculate the inverse of matrix A\n",
    "A_inv = np.linalg.inv(A)\n",
    "\n",
    "# Solve for the unknown vector x using matrix inversion: x = A_inv * b\n",
    "x = np.dot(A_inv, b)\n",
    "\n",
    "print(\"Solution using matrix inversion:\")\n",
    "print(\"x =\", x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eigenvalues and Eigenvectors\n",
    "\n",
    "- When a matrix acts on a vector, it generally **changes both its direction and length**. However, there are special vectors that **preserve their direction**—they are only **scaled** by the transformation. These are called **eigenvectors**, and the corresponding scaling factors are **eigenvalues**.\n",
    "\n",
    ":::{admonition} **Definition of Eigenvalue Problem**\n",
    ":class: important\n",
    "\n",
    "A vector $\\mathbf{v}$ is an **eigenvector** of a matrix $A$ if\n",
    "\n",
    "$$\n",
    "A \\mathbf{v} = \\lambda \\mathbf{v},\n",
    "$$\n",
    "\n",
    "where $\\lambda$ is a **scalar** called the **eigenvalue**.\n",
    ":::\n",
    "\n",
    "In this equation:\n",
    "\n",
    "* $A$ is the linear transformation (matrix or operator),\n",
    "* $\\mathbf{v}$ is a non-zero vector whose direction remains unchanged,\n",
    "* $\\lambda$ tells how much $\\mathbf{v}$ is stretched, compressed, or flipped.\n",
    "\n",
    "\n",
    "### Finding Eigenvalues\n",
    "\n",
    "Rearranging the equation gives:\n",
    "\n",
    "$$\n",
    "(A - \\lambda I)\\mathbf{v} = 0.\n",
    "$$\n",
    "\n",
    "For a nontrivial solution ($\\mathbf{v} \\neq 0$), the determinant must vanish:\n",
    "\n",
    "$$\n",
    "\\det(A - \\lambda I) = 0.\n",
    "$$\n",
    "\n",
    "This condition is the **characteristic equation** of the matrix and yields the possible eigenvalues $\\lambda$.\n",
    "\n",
    "\n",
    "### Geometric Meaning\n",
    "\n",
    "* If $\\lambda > 1$: the vector is **stretched**.\n",
    "* If $0 < \\lambda < 1$: the vector is **compressed**.\n",
    "* If $\\lambda < 0$: the vector is **flipped** in direction.\n",
    "* If $\\lambda = 0$: the vector is **collapsed** to the origin.\n",
    "\n",
    "In quantum mechanics, this idea generalizes to **operators acting on wavefunctions**—where eigenvalues correspond to **measurable quantities** (observables) and eigenvectors to the **states** with definite outcomes.\n",
    "\n",
    "\n",
    "| Property                | Expression                                       | Meaning                                            |\n",
    "| ----------------------- | ------------------------------------------------ | -------------------------------------------------- |\n",
    "| Eigenvalue equation     | $A\\mathbf{v} = \\lambda\\mathbf{v}$                | Defines scaling along eigenvector                  |\n",
    "| Inverse action          | $A^{-1}\\mathbf{v} = \\frac{1}{\\lambda}\\mathbf{v}$ | Inverse rescales in opposite way                   |\n",
    "| Invertibility condition | $\\det(A) \\neq 0$ or all $\\lambda \\neq 0$         | Matrix is invertible only if no eigenvalue is zero |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Visualizing Action of a Matrix on a Vector\n",
    "\n",
    "- These transformations illustrate different ways of modifying vectors and geometric shapes through scaling, shearing, and identity operations. We use a 2D vector and matrices as an example.\n",
    "\n",
    "| Matrix Type                  | Example                         | Effect                                        |\n",
    "| ---------------------------- | ------------------------------- | --------------------------------------------- |\n",
    "| Diagonal, unequal entries    | `[[2, 0], [0, 0.5]]`            | **Stretch** — different scaling per axis      |\n",
    "| Diagonal, equal entries      | `[[0.7, 0], [0, 0.7]]`          | **Uniform scaling** — isotropic compression   |\n",
    "| Orthogonal (rotation matrix) | `[[cosθ, -sinθ], [sinθ, cosθ]]` | **Rotation** — preserves lengths and angles   |\n",
    "| Off-diagonal elements        | `[[1, k], [0, 1]]`              | **Shear** — slants axes without changing area |\n",
    "\n",
    "- We pick the folloing four dummy vectors and act our matrix on all of them to see how the relationship between their distances and angles changes\n",
    "\n",
    "$$(0,0), (0.5,0.5), (0.5, 1.5), (0,1), (0,0)$$\n",
    "\n",
    "- We connect the points defined by this vectors to aid our visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Define the original shape (unit square)\n",
    "coords = np.array([\n",
    "    [0, 0],\n",
    "    [0.5, 0.5],\n",
    "    [0.5, 1.5],\n",
    "    [0, 1],\n",
    "    [0, 0]\n",
    "]).T\n",
    "\n",
    "# Define transformations\n",
    "stretch = np.array([[1.8, 0],\n",
    "                    [0, 0.8]])   # non-uniform diagonal → stretch\n",
    "rotation = np.array([[np.cos(np.pi/6), -np.sin(np.pi/6)],\n",
    "                     [np.sin(np.pi/6),  np.cos(np.pi/6)]])  # 30° rotation\n",
    "shear = np.array([[1, 0.8],\n",
    "                  [0, 1]])       # off-diagonal term → shear\n",
    "scaling = np.array([[0.7, 0],\n",
    "                    [0, 0.7]])   # uniform diagonal → scaling\n",
    "\n",
    "# Collect transformations\n",
    "transforms = {\n",
    "    \"Stretch (non-uniform scaling)\": stretch,\n",
    "    \"Rotation\": rotation,\n",
    "    \"Shear\": shear,\n",
    "    \"Scaling (uniform)\": scaling\n",
    "}\n",
    "\n",
    "# Set up 2x2 subplots\n",
    "fig, axes = plt.subplots(2, 2, figsize=(8, 8))\n",
    "\n",
    "for ax, (name, M) in zip(axes.flat, transforms.items()):\n",
    "    # Original shape\n",
    "    ax.plot(coords[0], coords[1], 'r--', label='original')\n",
    "    \n",
    "    # Transformed shape\n",
    "    new_coords = M @ coords\n",
    "    ax.plot(new_coords[0], new_coords[1], 'b-', label='transformed')\n",
    "    \n",
    "    # Style\n",
    "    ax.set_title(name, fontsize=11)\n",
    "    ax.set_aspect('equal', 'box')\n",
    "    ax.set_xlim(-2, 2)\n",
    "    ax.set_ylim(-1, 2)\n",
    "    ax.grid(True, ls=':')\n",
    "    ax.legend(fontsize=8, loc='upper left')\n",
    "\n",
    "plt.suptitle(\"Linear Transformations of a Vector Set\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
